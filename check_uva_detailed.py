#!/usr/bin/env python3
"""
–î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ UVA –∏ CUDA capabilities –≤ WSL2
"""

import torch
import subprocess
import sys

def check_cuda_basic():
    """–ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ CUDA"""
    print("=" * 80)
    print("üîç –ë–ê–ó–û–í–ê–Ø –ü–†–û–í–ï–†–ö–ê CUDA")
    print("=" * 80)
    print()
    
    if not torch.cuda.is_available():
        print("‚ùå CUDA –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞")
        return False
    
    print(f"‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞")
    print(f"   PyTorch: {torch.__version__}")
    print(f"   CUDA: {torch.version.cuda}")
    print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤: {torch.cuda.device_count()}")
    print()
    
    return True

def check_gpu_properties():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤–æ–π—Å—Ç–≤ GPU"""
    print("=" * 80)
    print("üìä –°–í–û–ô–°–¢–í–ê GPU")
    print("=" * 80)
    print()
    
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"GPU {i}: {props.name}")
        print(f"  Compute Capability: {props.major}.{props.minor}")
        print(f"  Total Memory: {props.total_memory / 1024**3:.2f} GB")
        print(f"  Multi Processors: {props.multi_processor_count}")
        
        # UVA —Ç—Ä–µ–±—É–µ—Ç Compute Capability >= 2.0
        if props.major >= 2:
            print(f"  ‚úÖ UVA Supported (CC >= 2.0)")
        else:
            print(f"  ‚ùå UVA Not Supported (CC < 2.0)")
        
        print()

def check_pinned_memory():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ pinned memory (—Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–ª—è UVA)"""
    print("=" * 80)
    print("üìå PINNED MEMORY")
    print("=" * 80)
    print()
    
    try:
        # –°–æ–∑–¥–∞–µ–º pinned memory
        x = torch.randn(1000, 1000).pin_memory()
        print("‚úÖ Pinned memory allocation works")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –ø–∞–º—è—Ç—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ pinned
        if x.is_pinned():
            print("‚úÖ Memory is pinned")
        else:
            print("‚ö†Ô∏è  Memory allocated but not pinned")
        
        # –¢–µ—Å—Ç –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ GPU
        x_gpu = x.cuda()
        print("‚úÖ Pinned memory -> GPU transfer works")
        
        del x, x_gpu
        print()
        return True
        
    except Exception as e:
        print(f"‚ùå Pinned memory error: {e}")
        print()
        return False

def check_unified_memory():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ Unified Memory (managed memory)"""
    print("=" * 80)
    print("üîó UNIFIED MEMORY (Managed Memory)")
    print("=" * 80)
    print()
    
    try:
        # Unified memory –≤ PyTorch
        # –≠—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ cudaMallocManaged
        import ctypes
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º CUDA runtime
        try:
            cuda = ctypes.CDLL("libcuda.so.1")
            print("‚úÖ CUDA runtime library accessible")
        except:
            print("‚ùå Cannot load libcuda.so.1")
            print()
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é –¥—Ä–∞–π–≤–µ—Ä–∞
        driver_version = ctypes.c_int()
        result = cuda.cuDriverGetVersion(ctypes.byref(driver_version))
        
        if result == 0:
            version = driver_version.value
            major = version // 1000
            minor = (version % 1000) // 10
            print(f"‚úÖ CUDA Driver Version: {major}.{minor}")
        else:
            print(f"‚ö†Ô∏è  Cannot get driver version (code: {result})")
        
        print()
        
        # Managed memory –æ–±—ã—á–Ω–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ WSL2
        print("‚ö†Ô∏è  Managed Memory (cudaMallocManaged) typically NOT supported in WSL2")
        print("   This is a known limitation")
        print()
        
        return False
        
    except Exception as e:
        print(f"‚ùå Unified memory check error: {e}")
        print()
        return False

def check_peer_access():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ peer-to-peer –¥–æ—Å—Ç—É–ø–∞ –º–µ–∂–¥—É —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º–∏"""
    print("=" * 80)
    print("üîÑ PEER-TO-PEER ACCESS")
    print("=" * 80)
    print()
    
    device_count = torch.cuda.device_count()
    
    if device_count < 2:
        print("‚ÑπÔ∏è  Only 1 GPU, P2P not applicable")
        print()
        return True
    
    for i in range(device_count):
        for j in range(device_count):
            if i != j:
                can_access = torch.cuda.can_device_access_peer(i, j)
                status = "‚úÖ" if can_access else "‚ùå"
                print(f"{status} GPU {i} -> GPU {j}: {can_access}")
    
    print()

def check_wsl_specific():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –¥–ª—è WSL –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π"""
    print("=" * 80)
    print("ü™ü WSL2 SPECIFIC CHECKS")
    print("=" * 80)
    print()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º—ã –≤ WSL
    try:
        with open("/proc/version", "r") as f:
            version = f.read()
            if "microsoft" in version.lower() or "wsl" in version.lower():
                print("‚úÖ Running in WSL2")
                print(f"   {version.strip()}")
            else:
                print("‚ÑπÔ∏è  Not running in WSL")
    except:
        pass
    
    print()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º nvidia-smi
    try:
        result = subprocess.run(
            ["nvidia-smi", "--query-gpu=driver_version", "--format=csv,noheader"],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            driver_ver = result.stdout.strip()
            print(f"‚úÖ NVIDIA Driver: {driver_ver}")
        else:
            print("‚ö†Ô∏è  nvidia-smi error")
    except:
        print("‚ùå nvidia-smi not available")
    
    print()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç–∏ –∫ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º
    import os
    wsl_lib_path = "/usr/lib/wsl/lib"
    if os.path.exists(wsl_lib_path):
        print(f"‚úÖ WSL CUDA libs found: {wsl_lib_path}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º libcuda.so
        libcuda = os.path.join(wsl_lib_path, "libcuda.so.1")
        if os.path.exists(libcuda):
            size = os.path.getsize(libcuda) / 1024**2
            print(f"   libcuda.so.1: {size:.1f} MB")
    else:
        print(f"‚ö†Ô∏è  WSL lib path not found: {wsl_lib_path}")
    
    print()

def check_vllm_requirements():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π vLLM –¥–ª—è CPU offloading"""
    print("=" * 80)
    print("üöÄ vLLM CPU OFFLOADING REQUIREMENTS")
    print("=" * 80)
    print()
    
    all_ok = True
    
    # 1. Compute Capability
    if torch.cuda.is_available():
        props = torch.cuda.get_device_properties(0)
        if props.major >= 2:
            print("‚úÖ Compute Capability >= 2.0 (UVA supported)")
        else:
            print("‚ùå Compute Capability < 2.0 (UVA not supported)")
            all_ok = False
    
    # 2. Pinned Memory
    try:
        x = torch.randn(100, 100).pin_memory()
        print("‚úÖ Pinned memory works")
    except:
        print("‚ùå Pinned memory doesn't work")
        all_ok = False
    
    # 3. Managed Memory (–æ–±—ã—á–Ω–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ WSL2)
    print("‚ùå Managed memory (cudaMallocManaged) - NOT supported in WSL2")
    print("   This is the MAIN blocker for vLLM V1 CPU offloading")
    all_ok = False
    
    print()
    
    if all_ok:
        print("üéâ All requirements met for CPU offloading")
    else:
        print("‚ö†Ô∏è  CPU offloading requirements NOT fully met")
        print()
        print("üí° Recommendations:")
        print("   1. Use vLLM WITHOUT --cpu-offload (you have 32GB VRAM)")
        print("   2. Use quantization if needed (AWQ/GPTQ)")
        print("   3. Use vLLM V0 with --disable-v1")
        print("   4. Use alternative: llama.cpp (better CPU offload support)")
    
    print()

def main():
    print()
    print("=" * 80)
    print("üîç COMPREHENSIVE UVA AND CUDA CAPABILITIES CHECK")
    print("=" * 80)
    print()
    
    if not check_cuda_basic():
        print("‚ùå CUDA not available, cannot proceed")
        sys.exit(1)
    
    check_gpu_properties()
    pinned_ok = check_pinned_memory()
    unified_ok = check_unified_memory()
    check_peer_access()
    check_wsl_specific()
    check_vllm_requirements()
    
    print("=" * 80)
    print("üìã SUMMARY")
    print("=" * 80)
    print()
    
    print(f"Pinned Memory:    {'‚úÖ' if pinned_ok else '‚ùå'}")
    print(f"Unified Memory:   {'‚úÖ' if unified_ok else '‚ùå'} (Expected ‚ùå in WSL2)")
    print()
    
    if not unified_ok:
        print("‚ö†Ô∏è  CONCLUSION:")
        print("   vLLM V1 CPU offloading will NOT work in WSL2")
        print("   due to lack of Unified Memory (cudaMallocManaged) support")
        print()
        print("‚úÖ SOLUTIONS:")
        print("   ‚Ä¢ Don't use --cpu-offload (you have enough VRAM)")
        print("   ‚Ä¢ Use --disable-v1 for legacy vLLM engine")
        print("   ‚Ä¢ Use quantization instead")
        print("   ‚Ä¢ Use llama.cpp for CPU offloading")
    
    print()

if __name__ == "__main__":
    main()