#!/usr/bin/env python3
"""
vLLM OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π API —Å–µ—Ä–≤–µ—Ä
–ó–∞–ø—É—Å–∫: python vllm_server.py
"""

import argparse
import os
from vllm.entrypoints.openai.api_server import run_server
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.entrypoints.openai.cli_args import make_arg_parser


def main():
    parser = argparse.ArgumentParser(description="vLLM OpenAI API Server")
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument("--model", type=str, 
                       default="Qwen/Qwen2.5-7B-Instruct",
                       help="–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace")
    
    parser.add_argument("--host", type=str, 
                       default="0.0.0.0",
                       help="Host –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞ (0.0.0.0 –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∏–∑ Windows)")
    
    parser.add_argument("--port", type=int, 
                       default=8000,
                       help="–ü–æ—Ä—Ç —Å–µ—Ä–≤–µ—Ä–∞")
    
    parser.add_argument("--gpu-memory-utilization", type=float, 
                       default=0.9,
                       help="–î–æ–ª—è GPU –ø–∞–º—è—Ç–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (0.0-1.0)")
    
    parser.add_argument("--max-model-len", type=int, 
                       default=None,
                       help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
    
    parser.add_argument("--tensor-parallel-size", type=int, 
                       default=1,
                       help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞")
    
    parser.add_argument("--trust-remote-code", action="store_true",
                       help="–î–æ–≤–µ—Ä—è—Ç—å —É–¥–∞–ª–µ–Ω–Ω–æ–º—É –∫–æ–¥—É (–¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª–µ–π)")
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("üöÄ –ó–∞–ø—É—Å–∫ vLLM API —Å–µ—Ä–≤–µ—Ä–∞")
    print("=" * 70)
    print(f"üì¶ –ú–æ–¥–µ–ª—å: {args.model}")
    print(f"üåê Host: {args.host}")
    print(f"üîå Port: {args.port}")
    print(f"üíæ GPU Memory: {args.gpu_memory_utilization * 100}%")
    print(f"üìè Max context: {args.max_model_len or 'auto'}")
    print("=" * 70)
    print(f"\n‚úÖ –°–µ—Ä–≤–µ—Ä –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É:")
    print(f"   WSL: http://localhost:{args.port}")
    print(f"   Windows: http://localhost:{args.port}")
    print(f"   –°–µ—Ç—å: http://<WSL_IP>:{args.port}")
    print("\nüìã OpenAI API endpoints:")
    print(f"   - http://localhost:{args.port}/v1/models")
    print(f"   - http://localhost:{args.port}/v1/completions")
    print(f"   - http://localhost:{args.port}/v1/chat/completions")
    print(f"   - http://localhost:{args.port}/docs (Swagger UI)")
    print("=" * 70)
    print("\n‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 30-60 —Å–µ–∫—É–Ω–¥)...\n")
    
    # –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ CLI (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π —Å–ø–æ—Å–æ–±)
    import sys
    sys.argv = [
        "vllm.entrypoints.openai.api_server",
        "--model", args.model,
        "--host", args.host,
        "--port", str(args.port),
        "--gpu-memory-utilization", str(args.gpu_memory_utilization),
        "--tensor-parallel-size", str(args.tensor_parallel_size),
    ]
    
    if args.max_model_len:
        sys.argv.extend(["--max-model-len", str(args.max_model_len)])
    
    if args.trust_remote_code:
        sys.argv.append("--trust-remote-code")
    
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä
    from vllm.entrypoints.openai.api_server import main as vllm_main
    vllm_main()


if __name__ == "__main__":
    main()
