#!/bin/bash

# vLLM —Å–µ—Ä–≤–µ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π

set -e

# –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
DEFAULT_MODEL="Qwen/Qwen2.5-7B-Instruct"
PORT=8000
HOST="0.0.0.0"
CPU_OFFLOAD_GB=""

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π: –Ω–∞–∑–≤–∞–Ω–∏–µ => "model_id|gpu_mem|max_len|quant|special_flags"
declare -A MODELS=(
    # Qwen –º–æ–¥–µ–ª–∏
    ["qwen-7b"]="Qwen/Qwen2.5-7B-Instruct|0.85|8192||"
    ["qwen-14b"]="Qwen/Qwen2.5-14B-Instruct|0.90|8192||"
    ["qwen-32b"]="Qwen/Qwen2.5-32B-Instruct|0.95|4096||"
    ["qwen-72b"]="Qwen/Qwen2.5-72B-Instruct-AWQ|0.95|4096|awq|"
    ["qwen-math-7b"]="Qwen/Qwen2.5-Math-7B-Instruct|0.85|4096||"
    ["qwen-math-72b"]="Qwen/Qwen2.5-Math-72B-Instruct-AWQ|0.95|4096|awq|"
    
    # Qwen Vision-Language –º–æ–¥–µ–ª–∏ (NEW!)
    ["qwen3-vl-2b"]="unsloth/Qwen3-VL-2B-Instruct|0.85|8192||vlm"
    
    # Llama –º–æ–¥–µ–ª–∏ (—Ç—Ä–µ–±—É—é—Ç gated access!)
    ["llama-3.2-3b"]="meta-llama/Llama-3.2-3B-Instruct|0.80|8192||"
    ["llama-3.1-8b"]="meta-llama/Llama-3.1-8B-Instruct|0.85|8192||"
    ["llama-3.3-70b"]="casperhansen/llama-3.3-70b-instruct-awq|0.85|8192|awq|"
    ["llama-3.1-70b"]="hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4|0.95|8192|awq|"
    
    # –î—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏
    ["mistral-7b"]="mistralai/Mistral-7B-Instruct-v0.3|0.85|8192||"
    ["mistral-small"]="mistralai/Mistral-Small-Instruct-2409|0.95|8192||"
    ["deepseek-math"]="deepseek-ai/deepseek-math-7b-instruct|0.85|8192||"
)

# –§—É–Ω–∫—Ü–∏—è –ø–æ–º–æ—â–∏
show_help() {
    cat << EOF
üöÄ vLLM Server Launcher

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: 
    ./start_server.sh [OPTIONS]

–û–ø—Ü–∏–∏:
    --model <name>           –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: qwen-7b)
    --port <port>            –ü–æ—Ä—Ç —Å–µ—Ä–≤–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 8000)
    --host <host>            Host –∞–¥—Ä–µ—Å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.0.0.0)
    --cpu-offload-gb <gb>    –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GB RAM –¥–ª—è CPU –æ—Ñ—Ñ–ª–æ–∞–¥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    --help                   –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç—É —Å–ø—Ä–∞–≤–∫—É

–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é –º–æ—â–Ω–æ—Å—Ç–∏):

üìä 2-3B –º–æ–¥–µ–ª–∏ (–æ—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–µ, ~4-6GB VRAM):
    qwen3-vl-2b       unsloth/Qwen3-VL-2B-Instruct [VLM]
                      Vision-Language –º–æ–¥–µ–ª—å, —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ + —Ç–µ–∫—Å—Ç–æ–º
                      –ö–æ–Ω—Ç–µ–∫—Å—Ç: 32K, –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–æ 10 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ø—Ä–æ–º–ø—Ç–µ

üìä 7B –º–æ–¥–µ–ª–∏ (–±—ã—Å—Ç—Ä—ã–µ, ~14GB VRAM):
    qwen-7b           Qwen/Qwen2.5-7B-Instruct
                      –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å, –æ—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä—É—Å—Å–∫–∏–º
                      
    qwen-math-7b      Qwen/Qwen2.5-Math-7B-Instruct
                      –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏–∫–µ
                      
    mistral-7b        mistralai/Mistral-7B-Instruct-v0.3
                      –ë—ã—Å—Ç—Ä–∞—è –∑–∞–ø–∞–¥–Ω–∞—è –º–æ–¥–µ–ª—å
                      
    deepseek-math     deepseek-ai/deepseek-math-7b-instruct
                      –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è

üìä 14B –º–æ–¥–µ–ª–∏ (–±–∞–ª–∞–Ω—Å, ~28GB VRAM):
    qwen-14b          Qwen/Qwen2.5-14B-Instruct
                      –£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ

üìä 24-32B –º–æ–¥–µ–ª–∏ (–º–æ—â–Ω—ã–µ, ~30GB VRAM):
    mistral-small     mistralai/Mistral-Small-Instruct-2409 (24B)
                      –ë–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞
                      
    qwen-32b          Qwen/Qwen2.5-32B-Instruct
                      –ú–∞–∫—Å–∏–º—É–º –±–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏

üìä 72B –º–æ–¥–µ–ª–∏ (–º–∞–∫—Å–∏–º—É–º, AWQ 4-bit, ~31GB VRAM):
    qwen-72b          Qwen/Qwen2.5-72B-Instruct-AWQ
                      –°–∞–º–∞—è –º–æ—â–Ω–∞—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
                      
    qwen-math-72b     Qwen/Qwen2.5-Math-72B-Instruct-AWQ
                      –õ—É—á—à–∞—è –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏

–ü—Ä–∏–º–µ—Ä—ã:
    ./start_server.sh
    ./start_server.sh --model qwen3-vl-2b
    ./start_server.sh --model qwen-math-72b
    ./start_server.sh --model qwen-14b --port 8001
    ./start_server.sh --model llama-3.3-70b --cpu-offload-gb 64
    
–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
    - –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: qwen-7b (–±—ã—Å—Ç—Ä–æ)
    - –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: qwen3-vl-2b (VLM)
    - –î–ª—è –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏: qwen-math-72b (–ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ)
    - –î–ª—è production: qwen-72b (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ—â—å)

–ü–æ—Å–ª–µ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –∞–¥—Ä–µ—Å—É:
    http://localhost:$PORT/v1

–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è API:
    http://localhost:$PORT/docs

–î–ª—è Vision –º–æ–¥–µ–ª–µ–π –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–æ—Ä–º–∞—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏:
    {
      "model": "vllm-model",
      "messages": [{
        "role": "user",
        "content": [
          {"type": "text", "text": "–ß—Ç–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ?"},
          {"type": "image_url", "image_url": {"url": "https://..."}}
        ]
      }]
    }

EOF
}

# –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
MODEL_NAME="qwen-7b"

while [[ $# -gt 0 ]]; do
    case $1 in
        --model)
            MODEL_NAME="$2"
            shift 2
            ;;
        --port)
            PORT="$2"
            shift 2
            ;;
        --host)
            HOST="$2"
            shift 2
            ;;
        --cpu-offload-gb)
            CPU_OFFLOAD_GB="$2"
            shift 2
            ;;
        --help|-h)
            show_help
            exit 0
            ;;
        *)
            echo "‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞—Ä–≥—É–º–µ–Ω—Ç: $1"
            echo "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --help –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏"
            exit 1
            ;;
    esac
done

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –º–æ–¥–µ–ª—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
if [[ ! -v MODELS[$MODEL_NAME] ]]; then
    echo "‚ùå –û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å '$MODEL_NAME' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
    echo ""
    echo "–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:"
    for key in "${!MODELS[@]}"; do
        echo "  - $key"
    done | sort
    echo ""
    echo "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --help –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
    exit 1
fi

# –†–∞–∑–±–æ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
IFS='|' read -r MODEL_ID GPU_MEM MAX_LEN QUANT SPECIAL <<< "${MODELS[$MODEL_NAME]}"

# –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –∫–æ–º–∞–Ω–¥—ã –∑–∞–ø—É—Å–∫–∞
CMD="python -m vllm.entrypoints.openai.api_server \
    --model $MODEL_ID \
    --host $HOST \
    --port $PORT \
    --trust-remote-code \
    --gpu-memory-utilization $GPU_MEM \
    --max-model-len $MAX_LEN \
    --served-model-name vllm-model"

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
if [[ -n "$QUANT" ]]; then
    CMD="$CMD --quantization $QUANT"
fi

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ CPU offload –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
if [[ -n "$CPU_OFFLOAD_GB" ]]; then
    CMD="$CMD --cpu-offload-gb $CPU_OFFLOAD_GB"
fi

# –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ–ª–∞–≥–∏ –¥–ª—è Vision-Language –º–æ–¥–µ–ª–µ–π
if [[ "$SPECIAL" == "vlm" ]]; then
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: JSON —Ñ–æ—Ä–º–∞—Ç –¥–ª—è --limit-mm-per-prompt
    CMD="$CMD --limit-mm-per-prompt '{\"image\": 10}'"
    CMD="$CMD --enable-prefix-caching"
    CMD="$CMD --enable-chunked-prefill"
    CMD="$CMD --max-num-batched-tokens 8192"
    IS_VLM=true
else
    IS_VLM=false
fi

# –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
echo "üöÄ –ó–∞–ø—É—Å–∫ vLLM —Å–µ—Ä–≤–µ—Ä–∞"
echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
echo "üì¶ –ú–æ–¥–µ–ª—å:        $MODEL_NAME"
echo "üîñ Model ID:      $MODEL_ID"
echo "üåê –ê–¥—Ä–µ—Å:         http://$HOST:$PORT"
echo "üíæ GPU Memory:    ${GPU_MEM}0%"
echo "üìè Max Length:    $MAX_LEN"

if [[ "$IS_VLM" == true ]]; then
    echo "üñºÔ∏è  Type:          Vision-Language Model (VLM)"
    echo "üì∏ Images:        –î–æ 10 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –ø—Ä–æ–º–ø—Ç–µ"
fi

if [[ -n "$QUANT" ]]; then
    echo "üî¢ Quantization:  $QUANT"
fi

if [[ -n "$CPU_OFFLOAD_GB" ]]; then
    echo "üíø CPU Offload:   ${CPU_OFFLOAD_GB} GB"
fi

echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
echo ""

# –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU
if command -v nvidia-smi &> /dev/null; then
    GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader | head -1)
    GPU_MEM_TOTAL=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
    echo "üéÆ GPU:           $GPU_NAME ($GPU_MEM_TOTAL MB)"
else
    echo "‚ö†Ô∏è  nvidia-smi –Ω–µ –Ω–∞–π–¥–µ–Ω"
fi

echo ""
echo "‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (30-60 —Å–µ–∫—É–Ω–¥)..."
echo ""

if [[ "$IS_VLM" == true ]]; then
    echo "üí° –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:"
    echo "   curl http://localhost:$PORT/v1/chat/completions \\"
    echo "     -H 'Content-Type: application/json' \\"
    echo "     -d '{"
    echo "       \"model\": \"vllm-model\","
    echo "       \"messages\": [{"
    echo "         \"role\": \"user\","
    echo "         \"content\": ["
    echo "           {\"type\": \"text\", \"text\": \"–ß—Ç–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ?\"},"
    echo "           {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://...\"}}"
    echo "         ]"
    echo "       }]"
    echo "     }'"
    echo ""
fi

echo "–î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C"
echo ""

# –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞
eval $CMD